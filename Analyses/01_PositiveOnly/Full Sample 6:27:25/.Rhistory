"Disaligned" = "#74B3CE"
)
) +
labs(
x = "Exposure Level (Frequency)",
y = "Mean Term Count",
color = "Ideological Alignment"
) +
theme_minimal(base_size = 14) +
theme(
panel.grid.major = element_line(color = "#eaeaea"),
panel.grid.minor = element_blank(),
panel.spacing = unit(1, "lines"),
strip.text = element_text(face = "bold", size = 13),
axis.title = element_text(face = "bold"),
axis.text = element_text(color = "#333333"),
legend.position = "bottom",
legend.title = element_text(face = "bold")
) +
scale_x_discrete(labels = c("Low Exposure", "High Exposure")) +
facet_grid(~itemPair)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("helpers.R")
source("survey.R")
library(jsonlite)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(stringdist)
library(lme4)
library(lmerTest)
library(brms)
paired_colors <- c(
"3"       = "#A56CC1", # purple
"aligned"       = "#DDA0DD", # light purple
"Aligned"       = "#DDA0DD", # light purple
"Republican"       = "#E76F51", # orange-red
"Domari"        = "#F4A261", # light orange
"disaligned" = "#2A9D8F", # teal
"Disaligned" = "#2A9D8F", # teal
"herdblurring"  = "#A8DADC", # light teal
"8"    = "#457B9D", # blue
"Democrat"    = "#A9CCE3"  # light blue
)
allData <- read.csv("sociopolitical_frequency_tradeoff_herdblurring_onl-merged.csv")
prolificIDs <- read.csv("sociopolitical_frequency_tradeoff_herdblurring_onl-workerids.csv")
prolificData <- read.csv("prolificData.csv") %>%
mutate(prolific_participant_id = paste(Participant.id)) %>%
select(c("prolific_participant_id","U.s..political.affiliation","Age","Sex"))
testData <- allData %>%
left_join(prolificIDs, by=c("workerid"))
fullData <- testData %>%
merge(prolificData, by=c("prolific_participant_id")) %>%
mutate(ProlificPolitical = paste(U.s..political.affiliation))
tweet_data <- fullData %>%
filter(category == "tweet_production") %>%
select(
workerid,
response,
completed_successfully,
failed_attempts,
max_attempts_reached,
required_word_1,
required_word_1_used,
required_word_2,
required_word_2_used
)
# Function to detect words with edit distance <= 2
check_words <- function(resp, words) {
words_found <- words[sapply(words, function(w) {
any(stringdist::stringdist(tolower(w), tolower(strsplit(resp, "\\s+")[[1]])) <= 2)
})]
if(length(words_found) == 0) return(character(0))  # return empty character vector instead of NA
words_found  # return as vector
}
`%!in%` <- function(x, y) ! (x %in% y)
# First, create a reference dataset with political context information
political_context <- fullData %>%
filter(!is.na(bias) & bias != "") %>%
select(workerid, term, bias, tokenCount, itemPair, ProlificPolitical)
# Create lookup tables for token count and alignment by critical term and worker
token_lookup <- political_context %>%
select(workerid, term, tokenCount,bias,ProlificPolitical) %>%
distinct()
delta_lookup <- token_lookup %>%
select(-term) %>%
filter(tokenCount != 0) %>%
pivot_wider(
names_from = bias,
values_from = tokenCount,
values_fn = sum,        # collapse if duplicates exist
values_fill = 0         # fill missing with 0
) %>%
mutate(delta = ifelse(right != left, TRUE, FALSE)) %>%
mutate(deltaType = case_when(
delta == FALSE & ProlificPolitical != "CONSENT_REVOKED" ~ paste(left),
delta == TRUE & ProlificPolitical == "Democrat" & left == 8 ~ "Aligned",
delta == TRUE & ProlificPolitical == "Democrat" & left == 3 ~ "Disaligned",
delta == TRUE & ProlificPolitical == "Republican" & right == 8 ~ "Aligned",
delta == TRUE & ProlificPolitical == "Republican" & right == 3 ~ "Disaligned",
TRUE ~ "error")) %>%
select(workerid, left, right, delta, deltaType)
lexicalDecisionFull <- fullData %>% filter(category == "LexicalDecision") %>% select(workerid, criticality, itemPair, response, rt, status, statusCheck, stimulus, time_elapsed, U.s..political.affiliation, Age, Sex, ProlificPolitical)  %>%
rename("term" = "stimulus") %>%
left_join(token_lookup, by=c("term", "workerid", "ProlificPolitical")) %>%
mutate(criticality = case_when(
itemPair == "privacy" ~ "critical",
TRUE ~ "distractor"
)) %>%
mutate(alignment = case_when(
ProlificPolitical == "Democrat" & bias == "left" ~ "aligned",
ProlificPolitical == "Republican" & bias == "right" ~ "aligned",
TRUE ~ "disaligned"
)) %>%
left_join(delta_lookup, by="workerid")
# Example visualization
ggplot(lexicalDecisionFull, aes(x = rt, fill = response)) +
geom_histogram(alpha = 0.7, bins = 30) +
theme_bw() +
labs(title = "Reaction Times by Response",
x = "Reaction Time (ms)",
y = "Count")
lexicalDecisionFull %>%
group_by(criticality) %>%
summarize(meanRt = (mean(rt))) %>%
ggplot(aes(x=criticality, y=meanRt)) +
geom_point() +
theme_bw()
lexicalDecisionFull %>%
filter(deltaType != "error") %>%
filter(criticality == "critical") %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, deltaType,delta,tokenCount) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x=alignment, y=meanRt, color = alignment, shape=as.factor(tokenCount))) +
geom_point() +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors) +
facet_wrap(delta~deltaType)
lexicalDecisionFull %>%
filter(delta == TRUE) %>%
filter(criticality == "critical") %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, tokenCount, delta) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x=as.factor(tokenCount), y=meanRt, color = alignment)) +
geom_point(position = position_dodge(0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)  +
labs(x="Exposure Count", color = "Alignment", y="Mean RT", title="Effect of Ideology and Frequency Single-Topic: Exp 2")
failureList <- allData %>%
filter(category == "failed_task") %>%
group_by(workerid) %>%
summarize(failedTimes = n())
lexicalDecision <- lexicalDecisionClean %>%
left_join(failureList, by="workerid") %>%
mutate(failedTimes = case_when(
is.na(failedTimes) ~ 0,
TRUE ~ failedTimes
))
lexicalDecision %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(tokenCount, failedTimes) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = failedTimes, y = meanRt, shape = as.factor(tokenCount))) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw()
lexicalDecisionFull %>%
filter(deltaType != "error") %>%
filter(criticality == "critical") %>%
group_by(tokenCount, alignment, deltaType) %>%
summarize(
meanRt = mean(rt, na.rm = TRUE),
n = n(),
seRt = sd(rt, na.rm = TRUE) / sqrt(n),
ci95 = qt(0.975, df = n - 1) * seRt
) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanRt, fill = alignment)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanRt - ci95, ymax = meanRt + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
facet_wrap(~deltaType)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(tokenCount, alignment, deltaType) %>%
mutate(accBinary = case_when(
statusCheck == "correct" ~ 1,
TRUE ~ 0,
)) %>%
summarize(
meanAcc = mean(accBinary, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanAcc, fill = alignment)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
theme_bw() +
facet_wrap(~deltaType)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
group_by(ProlificPolitical) %>%
mutate(accBinary = case_when(
statusCheck == "correct" ~ 1,
TRUE ~ 0,
)) %>%
summarize(
meanAcc = mean(accBinary, na.rm = TRUE),
n = n()) %>%
ggplot(aes(x =ProlificPolitical, y = meanAcc, fill = ProlificPolitical)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
theme_bw()
lexicalDecisionFull %>%
filter(deltaType != "error") %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, ProlificPolitical, tokenCount,deltaType) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = alignment, y = meanRt, shape = as.factor(tokenCount), color = ProlificPolitical)) +
geom_point(size = 3, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
facet_grid(deltaType~ProlificPolitical) +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(deltaType != "error") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(alignment, tokenCount,deltaType) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = alignment, y = meanRt, shape = as.factor(tokenCount), color = alignment)) +
geom_point(size = 4, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors) +
facet_wrap(~deltaType)
lexicalDecisionFull %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(ProlificPolitical) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = ProlificPolitical, y = meanRt, color = ProlificPolitical)) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors)
lexicalDecisionFull %>%
filter(deltaType != "error") %>%
filter(criticality == "critical") %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
# Trim RTs: remove <200ms and > 3 SDs from participant mean
mutate(rt_mean = mean(rt, na.rm = TRUE),
rt_sd = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
ungroup() %>%
group_by(tokenCount, deltaType) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n = sum(!is.na(rt_clean)),
seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = as.factor(tokenCount), y = meanRt, color = as.factor(tokenCount))) +
geom_point(size = 6, alpha = 0.4, position = position_dodge(width = 0.9)) +
geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
width = 0.2, position = position_dodge(width = 0.9)) +
theme_bw() +
scale_color_manual(values = paired_colors) +
facet_wrap(~deltaType)
lexicalDecisionFull %>%
filter(criticality == "critical",
ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
mutate(
rt_mean = mean(rt, na.rm = TRUE),
rt_sd   = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)
) %>%
ungroup() %>%
group_by(Age) %>%
summarize(
meanRt = mean(rt_clean, na.rm = TRUE),
n      = sum(!is.na(rt_clean)),
seRt   = sd(rt_clean, na.rm = TRUE) / sqrt(n),
.groups = "drop"
) %>%
ggplot(aes(x = Age, y = meanRt, group = 1)) +
geom_smooth(color = "steelblue", method="lm") +
geom_ribbon(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
alpha = 0.2, fill = "steelblue") +
labs(x = "Age", y = "Mean RT (ms)", title = "Mean Lexical Decision RT by Age") +
theme_bw()
lexicalDecisionFull %>%
group_by(ProlificPolitical) %>%
filter(!is.na(Age)) %>%
summarize(meanAge = mean(as.numeric(Age)))
# 1. Clean data
lexicalDecisionClean <- lexicalDecisionFull %>%
filter(criticality == "critical",
ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(workerid) %>%
mutate(
rt_mean  = mean(rt, na.rm = TRUE),
rt_sd    = sd(rt, na.rm = TRUE),
rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)
) %>%
ungroup() %>%
filter(!is.na(rt_clean))  # remove trimmed outliers
# 2. Fit a brms model
# We'll model rt_clean on alignment, ProlificPolitical, tokenCount, with random intercepts for workerid
brms_model <- brm(
formula = rt_clean ~ alignment * ProlificPolitical * tokenCount + (1 | workerid),
data = lexicalDecisionClean,
family = gaussian(),
chains = 4,
cores = 4,
iter = 4000,
warmup = 1000,
seed = 123
)
summary(brms_model, prob=0.89)
prodCoded <- tweet_data %>%
rowwise() %>%
mutate(
wordsUsed = list(check_words(response, c(required_word_1, required_word_2)))
) %>%
ungroup() %>%
left_join(token_lookup, by = c("workerid")) %>%
filter(wordsUsed %in% c('Herdblurring', 'Crowdcloaking', 'Visageveiling', 'Facefacading',
'Swarmshrouding', 'Mugmuddling', 'Huddlehiding', 'Buddyblanketing',
'Domaring', 'Churaking', 'Wenluring', 'Thumazing',
'Monzaling', 'Toonixing','Mutoling','Tikafing')) %>%
filter(term != "LuigiMangione") %>%
mutate(prodBinary = case_when(
term == wordsUsed ~ 1,
TRUE ~ 0
)) %>%
mutate(alignment = case_when(
ProlificPolitical == "Democrat" & bias == "left" ~ "aligned",
ProlificPolitical == "Republican" & bias == "right" ~ "aligned",
TRUE ~ "disaligned"
)) %>%
mutate(morphology = case_when(
term %in% c('Herdblurring', 'Crowdcloaking', 'Visageveiling', 'Facefacading',
'Swarmshrouding', 'Mugmuddling', 'Huddlehiding', 'Buddyblanketing') ~ "Compound",
TRUE ~ "Nonce"
)) %>%
left_join(delta_lookup, by="workerid")
prodCoded %>%
filter(deltaType != "error") %>%
group_by(alignment, tokenCount, deltaType) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = alignment, y = meanProd, fill = as.factor(tokenCount))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1) +
facet_wrap(~deltaType)
prodCoded %>%
group_by(morphology) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = morphology, y = meanProd)) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1)
fit <- brm(
prodBinary ~ alignment * tokenCount + (1|workerid) + (1|term),
data = prodCoded,
family = bernoulli(),
chains = 4, cores = 4, iter = 2000
)
summary(fit, prob=0.89)
prodCoded %>%
group_by(term) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n        = n(),
se       = sd(prodBinary, na.rm = TRUE) / sqrt(n),
.groups  = "drop"
) %>%
ggplot(aes(x = reorder(term, meanProd), y = meanProd, fill = term)) +
geom_col(position = position_dodge()) +
geom_errorbar(aes(ymin = meanProd - se, ymax = meanProd + se),
width = 0.2, position = position_dodge(width = 0.9)) +
coord_flip() +
labs(x = "Term", y = "Mean Production Rate") +
theme_bw() +
theme(legend.position = "none")
prodCoded %>%
filter(ProlificPolitical %in% c("Democrat", "Republican")) %>%
group_by(alignment, ProlificPolitical, tokenCount) %>%
summarize(
meanRt = mean((prodBinary), na.rm = TRUE),
n = n()) %>%
ggplot(aes(x=alignment, y=meanRt, shape=as.factor(tokenCount), color=ProlificPolitical)) +
geom_point(size=6, alpha=0.4, position=position_dodge(width = 0.9)) +
theme_bw() +
facet_wrap(~ProlificPolitical)
prodCoded %>%
filter(deltaType != "error") %>%
group_by(alignment, tokenCount) %>%
summarize(
meanProd = mean(prodBinary, na.rm = TRUE),
n = n(),
se = sqrt((meanProd * (1 - meanProd)) / n),
ci95 = qt(0.975, df = n - 1) * se
) %>%
ggplot(aes(x = alignment, y = meanProd, fill = as.factor(tokenCount))) +
geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanProd - ci95, ymax = meanProd + ci95),
width = 0.2,
position = position_dodge(width = 0.9)
) +
theme_bw() +
ylim(0, 1)
View(prodCoded)
