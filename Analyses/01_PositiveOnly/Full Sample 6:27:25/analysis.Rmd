---
title: "Papineau & Hawkins 2025- Analysis"
author: "Brandon Papineau"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
source("helpers.R")
source("survey.R")
library(jsonlite)

library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(stringdist)
library(lme4)
library(lmerTest)
library(brms)
```


```{r}
allData <- read.csv("sociopolitical_frequency_tradeoff_full_sample-trials.csv")

prolificIDs <- read.csv("sociopolitical_frequency_tradeoff_full_sample-workerids.csv")

prolificData <- read.csv("prolificData.csv") %>% 
  mutate(prolific_participant_id = paste(Participant.id)) %>% 
  select(c("prolific_participant_id","U.s..political.affiliation","Age","Sex"))

testData <- allData %>%
  left_join(prolificIDs, by=c("workerid"))

fullData <- testData %>% 
  merge(prolificData, by=c("prolific_participant_id")) %>% 
  mutate(ProlificPolitical = paste(U.s..political.affiliation))

prodCoded <- read.csv("production_data_export_coded.csv") %>% 
  mutate(inclusion = ifelse(is.na(inclusion),"TRUE",paste(inclusion)))
  # filter(inclusion == "TRUE") %>%
  # filter(!str_detect(criticalTerm, ","))

# First, create a reference dataset with political context information
political_context <- fullData %>%
  filter(!is.na(wingBias) & wingBias != "") %>%
  select(workerid, criticalTerm, wingBias, tokenCount, itemPair, ProlificPolitical)

# Create lookup tables for token count and alignment by critical term and worker
token_lookup <- political_context %>%
  select(workerid, criticalTerm, tokenCount,wingBias, itemPair,ProlificPolitical) %>%
  filter(criticalTerm %in% c("crowdcloaking","herdblurring","Thumaze","Wenlure","Churako","Domari","tessamorph","interforme")) %>% 
  mutate(itemPair = case_when(
  criticalTerm %in% c("Churako", "Domari") ~ "martialArts",
  criticalTerm %in% c("Thumaze", "Wenlure") ~ "drugs",
  criticalTerm %in% c("interforme", "tessamorph") ~ "tattoos",
  criticalTerm %in% c("crowdcloaking", "herdblurring") ~ "privacy",
  TRUE ~ "error"
)) %>% 
  distinct() 

patternLookup <- token_lookup %>% 
  mutate(
    wingBias = ifelse(wingBias == "right", "Republican", "Democrat"),
    alignment = ifelse(wingBias == ProlificPolitical, "Aligned", "Disaligned")
  ) %>% 
  filter(tokenCount == 8) %>%  # keep only high-exposure terms
  group_by(workerid) %>%
  summarize(
    n_high_terms = n(),
    sides_of_high = n_distinct(alignment),
    dominant_alignment = if (n_distinct(alignment) == 1) first(alignment) else NA_character_,
    pattern = case_when(
      sides_of_high == 1 ~ "SameSide",
      sides_of_high == 2 ~ "SplitSides",
      TRUE ~ "Other"
    ),
    .groups = "drop"
  ) %>%
  mutate(
    # Add subpattern for SameSide people
    pattern_detail = case_when(
      pattern == "SameSide" & dominant_alignment == "Aligned" ~ "SameSide_Aligned",
      pattern == "SameSide" & dominant_alignment == "Disaligned" ~ "SameSide_Disaligned",
      TRUE ~ pattern
    )
  )

patterns <- c("crowdcloaking","herdblurring","Thumaze","Wenlure","Churako","Domari","tessamorph","interforme")
pattern_regex <- str_c(patterns, collapse = "|")

prodCodedFull <- prodCoded %>% 
  filter(!notes %in% c("LLM","duplicate","swap","mix","nonsense","wrongMapping")) %>% 
  left_join(token_lookup, by=c("workerid","itemPair")) %>% 
  mutate(wingBias = ifelse(wingBias == "right","Republican","Democrat"),
       alignment = ifelse(wingBias == ProlificPolitical,"Aligned","Disaligned")) %>% 
  left_join(patternLookup, by = "workerid") %>% 
  mutate(
    criticalTerm.x_list = str_split(tolower(criticalTerm.x), ",\\s*"),
    termUsed = mapply(function(y, x_list) tolower(y) %in% x_list, criticalTerm.y, criticalTerm.x_list)
  ) %>%
  mutate(termUsed = as.integer(termUsed)) # %>% 
 # mutate(isLLM = ifelse(notes == "LLM",TRUE,FALSE))

includeList <- unique(prodCodedFull$workerid)


lexicalDecisionFull <- fullData %>% 
  filter(workerid %in% includeList) %>% 
  filter(category == "LexicalDecision") %>% 
  filter(itemPair %in% c("tattoos","privacy","martialArts","drugs")) %>% 
  mutate(criticalTerm = stimulus) %>% 
  left_join(token_lookup, by=c("workerid","criticalTerm")) %>% 
  mutate(wingBias = ifelse(wingBias.y == "right","Republican","Democrat"),
       alignment = ifelse(wingBias == ProlificPolitical.x,"Aligned","Disaligned")) %>% 
  left_join(patternLookup, by = "workerid") 

lexicalDecisionFull %>% 
  filter(!is.na(alignment)) %>% 
  filter(itemPair.x == 'privacy') %>% 
  group_by(alignment,tokenCount.y) %>% 
  summarize(
    meanRt = mean(rt, na.rm = TRUE),
    se = sd(rt, na.rm = TRUE) / sqrt(n()),  # standard error
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = as.factor(tokenCount.y), y = meanRt, color = alignment)) + 
  geom_point(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_errorbar(
    aes(ymin = meanRt - se, ymax = meanRt + se),
    position = position_dodge(width = 0.9),
    width = 0.2
  ) +
  labs(
    y = "Mean RT",
    x = "Exposure"
  ) +
  theme_minimal() + 
  # facet_wrap(
  #   ~pattern,
  #   labeller = as_labeller(c(
  #     "SameSide" = "Isomorphic",
  #     "SplitSides" = "Nonisomorphic"
  #   ))
  # ) +
    scale_color_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  scale_x_discrete(labels = c("Low Exposure", "High Exposure"))
```

```{r}
lexicalDecisionFull %>% 
  filter(criticality == "critical") %>% 
  filter(itemPair.x == 'privacy') %>% 
  group_by(workerid) %>%
  # Trim RTs: remove <200ms and > 3 SDs from participant mean
  mutate(rt_mean = mean(rt, na.rm = TRUE),
         rt_sd = sd(rt, na.rm = TRUE),
         rt_clean = ifelse(rt < 200 | rt > rt_mean + 3 * rt_sd, NA, rt)) %>%
  ungroup() %>%
  group_by(alignment, tokenCount.y) %>%
  summarize(
    meanRt = mean(rt_clean, na.rm = TRUE),
    n = sum(!is.na(rt_clean)),
    seRt = sd(rt_clean, na.rm = TRUE) / sqrt(n),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x=as.factor(tokenCount.y), y=meanRt, color = alignment)) + 
  geom_point(position = position_dodge(0.9)) + 
  geom_errorbar(aes(ymin = meanRt - seRt, ymax = meanRt + seRt),
              width = 0.2, position = position_dodge(width = 0.9)) +
  theme_bw() + 
  scale_color_manual(values = paired_colors)  + 
  labs(x="Exposure Count", color = "Alignment", y="Mean RT", title="Effect of Ideology and Frequency Single-Topic: Exp 1")
```


```{r}
lexicalDecisionFull %>% 
  filter(!is.na(alignment)) %>%
  group_by(tokenCount.y,pattern,alignment) %>% 
  summarize(
    meanRt = mean(rt, na.rm = TRUE),
    se = sd(rt, na.rm = TRUE) / sqrt(n()),  # standard error
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = as.factor(tokenCount.y), y = meanRt, color = alignment)) + 
  geom_point(stat = "identity", position = position_dodge(width = 0.9)) +
  geom_errorbar(
    aes(ymin = meanRt - se, ymax = meanRt + se),
    position = position_dodge(width = 0.9),
    width = 0.2
  ) +
  labs(
    y = "Mean RT",
    x = "Exposure"
  ) +
  theme_minimal() + 
  # facet_wrap(
  #   ~pattern,
  #   labeller = as_labeller(c(
  #     "SameSide" = "Isomorphic",
  #     "SplitSides" = "Nonisomorphic"
  #   ))
  # ) +
    scale_color_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  scale_x_discrete(labels = c("Low Exposure", "High Exposure")) +
  facet_grid(~pattern, labeller = labeller(
    pattern = c(
      "SameSide" = "Isomorphic Frequency-Alignment",
      "SplitSides" = "Nonisomorphic Frequency-Alignment"
    )
  ))
```


```{r}
# Group by alignment and tokenCount, then count how many participants produced each type
prodGrouped <- prodCodedFull %>%
  filter(termUsed == 1) %>% 
  group_by(alignment, tokenCount, pattern) %>%
  summarise(count = n(), .groups = "drop") %>% 
  mutate(
    tokenCount = ifelse(tokenCount == 8, "High Exposure", "Low Exposure"),
    pattern = ifelse(pattern == "SameSide", "Isomorphic", "Non-Isomorphic")
  ) 

# Plot
prodGrouped %>%
  ggplot(aes(x = factor(tokenCount), y = count, fill = alignment)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(
    aes(label = count),
    position = position_dodge(width = 0.8),
    vjust = -0.3,
    size = 4,
    color = "black"
  ) +
  labs(
    x = "Term Exposure",
    y = "Number of First Productions",
    fill = "Term Alignment"
  ) +
  scale_x_discrete(labels = c("Low Exposure", "High Exposure")) +
  scale_fill_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "top",
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  ) +
  expand_limits(y = max(prodGrouped$count) * 1.1) +
  facet_grid(~pattern, labeller = labeller(
    pattern = c(
      "Isomorphic" = "Isomorphic Frequency-Alignment",
      "Non-Isomorphic" = "Nonisomorphic Frequency-Alignment"
    )
  ))
```

Note: of 800 participant trials (400x2/participant), participants produced 555 valid productions (all <= 2 edit distance from ideal orthographic form)


```{r}
summary_df <- prodCodedFull %>%
  group_by(pattern, alignment, tokenCount) %>%
  summarize(
    mean_count = mean(termUsed),
    se = sd(termUsed) / sqrt(n()),
    .groups = "drop"
  )

ggplot(summary_df, aes(x = factor(tokenCount), y = mean_count, color = alignment, group = alignment)) + 
  geom_point(
    position = position_dodge(0.5),
    size = 3,
    shape = 21,
    fill = "white",
    stroke = 1.2
  ) +
  geom_errorbar(
    aes(ymin = mean_count - se, ymax = mean_count + se),
    position = position_dodge(0.5),
    width = 0.2
  ) +
  geom_line(
    position = position_dodge(0.5),
    size = 1
  ) +
  scale_color_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  labs(
    x = "Exposure Level (Frequency)",
    y = "Mean Term Count",
    color = "Alignment"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major = element_line(color = "#eaeaea"),
    panel.grid.minor = element_blank(),
    panel.spacing = unit(1, "lines"),
    strip.text = element_text(face = "bold", size = 13),
    axis.title = element_text(face = "bold"),
    axis.text = element_text(color = "#333333"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  ) + 
  facet_grid(~pattern, labeller = labeller(
    pattern = c(
      "SameSide" = "Isomorphic Freq-Alignment",
      "SplitSides" = "Nonisomorphic Freq-Alignment"
    )
  )) + 
   scale_x_discrete(labels = c("Low Exposure", "High Exposure"))

```


```{r}
library(brms)

# Center contrasts
prodCodedFull$tokenCount <- factor(prodCodedFull$tokenCount)
prodCodedFull$pattern <- factor(prodCodedFull$pattern)
contrasts(prodCodedFull$tokenCount) <- contr.sum(2)
contrasts(prodCodedFull$pattern) <- contr.sum(2)

# Fit model
fit_full <- brm(
  formula = termUsed ~ tokenCount * alignment + 
    (1 + tokenCount || workerid) + 
    (1 | itemPair), 
  family = bernoulli(link = "logit"),
  data = prodCodedFull,
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(student_t(3, 0, 2.5), class = "Intercept")
  ),
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 1000,
  control = list(adapt_delta = 0.995),
  save_pars = save_pars(all = TRUE)  # for bridgesampling
)


summary(fit_full, prob = 0.95)
```


```{r}
library(tidybayes)
library(dplyr)
library(ggplot2)

# Ensure factor levels match model's data
prodCodedFull$tokenCount <- factor(prodCodedFull$tokenCount)
prodCodedFull$alignment <- factor(prodCodedFull$alignment)

newdata <- expand.grid(
  tokenCount = levels(prodCodedFull$tokenCount),
  alignment  = levels(prodCodedFull$alignment)
)

# Get posterior predictions
preds <- add_epred_draws(
  fit_full,
  newdata = newdata,
  re_formula = NA
)

# Summarise raw data for overlay
raw_props <- prodCodedFull %>%
  group_by(tokenCount, alignment) %>%
  summarise(prop = mean(termUsed), .groups = "drop")

# Plot
ggplot(preds, aes(x = tokenCount, y = .epred,
                  color = alignment, fill = alignment)) +
  stat_lineribbon(.width = 0.95, alpha = 0.2, size = 0) +
  stat_summary(fun = mean, geom = "line", aes(group = alignment), size = 1) +
  geom_point(data = raw_props,
             aes(x = tokenCount, y = prop, color = alignment),
             position = position_dodge(width = 0.2),
             size = 3, shape = 21, fill = "white") +
  labs(
    x = "Token Count (Exposure Frequency)",
    y = "Predicted Probability of Term Use",
    color = "Alignment",
    fill = "Alignment"
  ) +
  theme_minimal(base_size = 14)

```


```{r}
summary_df <- prodCodedFull %>%
  filter(!is.na(criticalTerm.x)) %>% 
  group_by(alignment, tokenCount,itemPair) %>%
  summarize(
    count = n(),
    mean_count = mean(termUsed),
    se = sd(termUsed) / sqrt(n()),
    .groups = "drop"
  ) %>% 
  filter(count > 5)

ggplot(summary_df, aes(x = factor(tokenCount), y = mean_count, color = alignment, group = alignment)) + 
  geom_point(
    position = position_dodge(0.5),
    size = 3,
    shape = 21,
    fill = "white",
    stroke = 1.2
  ) +
  geom_errorbar(
    aes(ymin = mean_count - se, ymax = mean_count + se),
    position = position_dodge(0.5),
    width = 0.2
  ) +
  geom_line(
    position = position_dodge(0.5),
    size = 1
  ) +
  scale_color_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  labs(
    x = "Exposure Level (Frequency)",
    y = "Mean Term Count",
    color = "Ideological Alignment"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major = element_line(color = "#eaeaea"),
    panel.grid.minor = element_blank(),
    panel.spacing = unit(1, "lines"),
    strip.text = element_text(face = "bold", size = 13),
    axis.title = element_text(face = "bold"),
    axis.text = element_text(color = "#333333"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  ) + 
   scale_x_discrete(labels = c("Low Exposure", "High Exposure")) +
  facet_grid(~itemPair)

```


```{r}
# Summarise: mean probability of termUsed per target term
term_memorability <- prodCodedFull %>%
  group_by(targetTerm = tolower(criticalTerm.y)) %>% # 
  summarise(
    mean_use = mean(termUsed, na.rm = TRUE),
    n = n()
  ) %>%
  arrange(desc(mean_use))

# Plot
ggplot(term_memorability, aes(x = reorder(targetTerm, mean_use), y = mean_use)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = scales::percent(mean_use, accuracy = 1)),
            vjust = -0.5, size = 4) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    x = "Target Term",
    y = "Proportion of Times Term Was Used",
    title = "Baseline Memorability of Each Term"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r}
# Binomial standard error helper
binomial_se <- function(p, n) sqrt(p * (1 - p) / n)

# Summarise mean proportion and SE by group
term_memorability <- prodCodedFull %>%
  # filter(!isLLM) %>%
  # filter(itemPair == "martialArts") %>% 
  group_by(itemPair, criticalTerm.y,tokenCount) %>%
  summarise(
    mean_use = mean(termUsed, na.rm = TRUE),
    n = n(),
    se = binomial_se(mean_use, n),
    .groups = "drop"
  ) %>%
  filter(n > 0)  # drop empty bars

# Define paired colors for each itemPair
paired_colors <- c(
  "Thumaze"       = "#A56CC1", # purple
  "Wenlure"       = "#DDA0DD", # light purple
  "Churako"       = "#E76F51", # orange-red
  "Domari"        = "#F4A261", # light orange
  "crowdcloaking" = "#2A9D8F", # teal
  "herdblurring"  = "#A8DADC", # light teal
  "interforme"    = "#457B9D", # blue
  "tessamorph"    = "#A9CCE3"  # light blue
)

# Plot
ggplot(term_memorability,
       aes(x = criticalTerm.y, y = mean_use, fill = criticalTerm.y)) +
  geom_col(width = 0.75) +
  geom_errorbar(
    aes(ymin = mean_use - se, ymax = mean_use + se),
    width = 0.25,
    linewidth = 0.6
  ) +
  scale_fill_manual(values = paired_colors) +
  scale_y_continuous(labels = label_percent(accuracy = 1),
                     expand = expansion(mult = c(0, 0.05))) +
  labs(
    x = "Target Term",
    y = "Proportion of Times Term Was Used",
    title = "Memorability of Terms by Item Pair",
    fill = "Target Term"
  ) +
  facet_grid(tokenCount ~ itemPair, scales = "free_x", space = "free_x") +
  theme_bw(base_size = 14) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    axis.text.x = element_text(angle = 0, hjust = 1)
  )

```


```{r}
lexicalDecisionFull %>% 
  group_by(criticalTerm,criticality) %>% 
  summarize(meanRt = mean(log(rt))) %>% 
  ggplot(aes(x=criticalTerm,y=meanRt, color=criticalTerm, shape=criticality)) +
  geom_point(stat="identity", size=3) + 
  theme_bw() + 
  scale_color_paletteer_d("ggthemes::Classic_Cyclic") + 
  labs(x="Novel Term", y="Mean Decision Time (log)", shape="Stim Type") + 
  guides(color="none") +
  theme(axis.text.x = element_text(angle=45, vjust = 0.5))
  
```


```{r}
summary_df <- prodCodedFull %>%
  # left_join(patternLookup %>% select(workerid, pattern_detail), by = "workerid") %>%
  group_by(pattern_detail, alignment, tokenCount) %>%
  summarize(
    mean_count = mean(termUsed),
    se = sd(termUsed) / sqrt(n()),
    .groups = "drop"
  )

ggplot(summary_df, aes(x = factor(tokenCount), y = mean_count, color = alignment, group = alignment)) + 
  geom_point(
    position = position_dodge(0.5),
    size = 3,
    shape = 21,
    fill = "white",
    stroke = 1.2
  ) +
  geom_errorbar(
    aes(ymin = mean_count - se, ymax = mean_count + se),
    position = position_dodge(0.5),
    width = 0.2
  ) +
  geom_line(
    position = position_dodge(0.5),
    size = 1
  ) +
  scale_color_manual(
    values = c(
      "Aligned" = "#8D6A9F",
      "Disaligned" = "#74B3CE"
    )
  ) +
  labs(
    x = "Exposure Level",
    y = "Mean Term Count",
    color = "Ideological Alignment"
  ) +
  theme_bw(base_size = 14) +
  theme(
    panel.grid.major = element_line(color = "#eaeaea"),
    panel.grid.minor = element_blank(),
    panel.spacing = unit(1, "lines"),
    strip.text = element_text(face = "bold", size = 13),
    axis.title = element_text(face = "bold"),
    axis.text = element_text(color = "#333333"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  ) + 
  facet_grid(~pattern_detail, labeller = labeller(
    pattern_detail = c(
      "SameSide_Aligned" = "Isomorphic In-Group",
      "SameSide_Disaligned" = "Isomorphic Out-Group",
      "SplitSides" = "Nonisomorphic"
    )
  )) + 
  scale_x_discrete(labels = c("Low", "High"))

```

# Explore

```{r}
learningData <- fullData %>% 
  filter(category=="tweet_trial") %>% 
  filter(workerid %in% includeList) %>% 
  

learningData %>% 
  group_by(persona, response) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x=persona, y=count, fill=response)) + 
  geom_bar(stat='identity', position=position_dodge()) + 
  scale_fill_paletteer_d("tvthemes::Bismuth") + 
  theme_bw()
```

```{r}

```

