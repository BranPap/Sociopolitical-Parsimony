knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("helpers.R")
source("survey.R")
library(jsonlite)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
library(stringdist)
library(lme4)
library(lmerTest)
library(brms)
allData <- read.csv("sociopolitical_frequency_tradeoff_full_sample-trials.csv")
prolificIDs <- read.csv("sociopolitical_frequency_tradeoff_full_sample-workerids.csv")
prolificData <- read.csv("prolificData.csv") %>%
mutate(prolific_participant_id = paste(Participant.id)) %>%
select(c("prolific_participant_id","U.s..political.affiliation","Age","Sex"))
testData <- allData %>%
left_join(prolificIDs, by=c("workerid"))
fullData <- testData %>%
merge(prolificData, by=c("prolific_participant_id")) %>%
mutate(ProlificPolitical = paste(U.s..political.affiliation))
prodCoded <- read.csv("production_data_export_coded.csv") %>%
mutate(inclusion = ifelse(is.na(inclusion),"TRUE",paste(inclusion)))
# filter(inclusion == "TRUE") %>%
# filter(!str_detect(criticalTerm, ","))
# First, create a reference dataset with political context information
political_context <- fullData %>%
filter(!is.na(wingBias) & wingBias != "") %>%
select(workerid, criticalTerm, wingBias, tokenCount, itemPair, ProlificPolitical)
# Create lookup tables for token count and alignment by critical term and worker
token_lookup <- political_context %>%
select(workerid, criticalTerm, tokenCount,wingBias, itemPair,ProlificPolitical) %>%
filter(criticalTerm %in% c("crowdcloaking","herdblurring","Thumaze","Wenlure","Churako","Domari","tessamorph","interforme")) %>%
mutate(itemPair = case_when(
criticalTerm %in% c("Churako", "Domari") ~ "martialArts",
criticalTerm %in% c("Thumaze", "Wenlure") ~ "drugs",
criticalTerm %in% c("interforme", "tessamorph") ~ "tattoos",
criticalTerm %in% c("crowdcloaking", "herdblurring") ~ "privacy",
TRUE ~ "error"
)) %>%
distinct()
patternLookup <- token_lookup %>%
mutate(
wingBias = ifelse(wingBias == "right", "Republican", "Democrat"),
alignment = ifelse(wingBias == ProlificPolitical, "Aligned", "Disaligned")
) %>%
filter(tokenCount == 8) %>%  # keep only high-exposure terms
group_by(workerid) %>%
summarize(
n_high_terms = n(),
sides_of_high = n_distinct(alignment),
dominant_alignment = if (n_distinct(alignment) == 1) first(alignment) else NA_character_,
pattern = case_when(
sides_of_high == 1 ~ "SameSide",
sides_of_high == 2 ~ "SplitSides",
TRUE ~ "Other"
),
.groups = "drop"
) %>%
mutate(
# Add subpattern for SameSide people
pattern_detail = case_when(
pattern == "SameSide" & dominant_alignment == "Aligned" ~ "SameSide_Aligned",
pattern == "SameSide" & dominant_alignment == "Disaligned" ~ "SameSide_Disaligned",
TRUE ~ pattern
)
)
patterns <- c("crowdcloaking","herdblurring","Thumaze","Wenlure","Churako","Domari","tessamorph","interforme")
pattern_regex <- str_c(patterns, collapse = "|")
prodCodedFull <- prodCoded %>%
filter(!notes %in% c("LLM","duplicate","swap","mix","nonsense","wrongMapping")) %>%
left_join(token_lookup, by=c("workerid","itemPair")) %>%
mutate(wingBias = ifelse(wingBias == "right","Republican","Democrat"),
alignment = ifelse(wingBias == ProlificPolitical,"Aligned","Disaligned")) %>%
left_join(patternLookup, by = "workerid") %>%
mutate(
criticalTerm.x_list = str_split(tolower(criticalTerm.x), ",\\s*"),
termUsed = mapply(function(y, x_list) tolower(y) %in% x_list, criticalTerm.y, criticalTerm.x_list)
) %>%
mutate(termUsed = as.integer(termUsed)) # %>%
# mutate(isLLM = ifelse(notes == "LLM",TRUE,FALSE))
includeList <- unique(prodCodedFull$workerid)
lexicalDecisionFull <- fullData %>%
filter(workerid %in% includeList) %>%
filter(category == "LexicalDecision") %>%
filter(itemPair %in% c("tattoos","privacy","martialArts","drugs")) %>%
mutate(criticalTerm = stimulus) %>%
left_join(token_lookup, by=c("workerid","criticalTerm")) %>%
mutate(wingBias = ifelse(wingBias.y == "right","Republican","Democrat"),
alignment = ifelse(wingBias == ProlificPolitical.x,"Aligned","Disaligned")) %>%
left_join(patternLookup, by = "workerid")
lexicalDecisionFull %>%
filter(!is.na(alignment)) %>%
group_by(alignment,tokenCount.y) %>%
summarize(
meanRt = mean(rt, na.rm = TRUE),
se = sd(rt, na.rm = TRUE) / sqrt(n()),  # standard error
.groups = "drop"
) %>%
ggplot(aes(x = as.factor(tokenCount.y), y = meanRt, color = alignment)) +
geom_point(stat = "identity", position = position_dodge(width = 0.9)) +
geom_errorbar(
aes(ymin = meanRt - se, ymax = meanRt + se),
position = position_dodge(width = 0.9),
width = 0.2
) +
labs(
y = "Mean RT",
x = "Exposure"
) +
theme_minimal() +
# facet_wrap(
#   ~pattern,
#   labeller = as_labeller(c(
#     "SameSide" = "Isomorphic",
#     "SplitSides" = "Nonisomorphic"
#   ))
# ) +
scale_color_manual(
values = c(
"Aligned" = "#8D6A9F",
"Disaligned" = "#74B3CE"
)
) +
scale_x_discrete(labels = c("Low Exposure", "High Exposure"))
View(lexicalDecisionFull)
View(fullData)
